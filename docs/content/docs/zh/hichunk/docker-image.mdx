---
title: 使用 Docker 部署 HiChunk
---

本文档提供构建包含所有模型权重和依赖项的自包含 HiChunk Docker 镜像的说明。

**环境要求：**
- 系统已安装 Docker
- 足够的磁盘空间（镜像约 10GB）
- 支持 CUDA 12.x 的 NVIDIA GPU（用于运行容器）

# 设置构建目录

首先，创建用于构建 Docker 镜像的目录并下载模型权重：

```bash
mkdir hichunk-docker && cd hichunk-docker

# 下载模型权重
git lfs install
git clone https://huggingface.co/tencent/HiChunk
```

# 创建自定义 vLLM 模型文件

HiChunk 需要在 vLLM 中注册自定义模型文件。在构建目录中创建以下文件。

## utu_v1.py

创建名为 `utu_v1.py` 的文件，包含 HiChunk 模型实现。您可以从下载的 `HiChunk` 目录或[本地部署指南](/docs/zh/hichunk/deploying_locally)复制此文件。

```bash
# 从下载的模型目录复制
cp HiChunk/utu_v1.py ./utu_v1.py
```

## registry.py

创建名为 `registry.py` 的文件，包含更新的 vLLM 模型注册表。您可以从下载的 `HiChunk` 目录或[本地部署指南](/docs/zh/hichunk/deploying_locally)复制此文件。

```bash
# 从下载的模型目录复制
cp HiChunk/registry.py ./registry.py
```

# 创建 Dockerfile

创建名为 `Dockerfile` 的文件，内容如下：

```dockerfile
# Use the official vLLM image with CUDA 12.x support
FROM vllm/vllm-openai:v0.9.1

# Set working directory
WORKDIR /app

# Install additional dependencies
RUN pip install --no-cache-dir liger_kernel transformers==4.53.0

# Copy the custom vLLM model files to the correct location
COPY utu_v1.py /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utu_v1.py
COPY registry.py /usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/registry.py

# Copy the model weights into the container
COPY HiChunk/ /app/HiChunk/

# Set environment variables
ENV MODEL_PATH=/app/HiChunk
ENV PORT=8501

# Expose the server port
EXPOSE 8501

# Set the entrypoint to run vLLM serve
ENTRYPOINT ["python", "-m", "vllm.entrypoints.openai.api_server"]

# Default command arguments
CMD ["--model", "/app/HiChunk", \
     "--served-model-name", "HiChunk", \
     "--port", "8501", \
     "--host", "0.0.0.0", \
     "--trust-remote-code", \
     "--dtype", "bfloat16", \
     "--max-num-batched-tokens", "32768", \
     "--enforce-eager", \
     "--seed", "0"]
```

# 构建 Docker 镜像

使用以下命令构建 Docker 镜像：

```bash
docker build -t hichunk:latest .
```

此过程可能需要几分钟，因为它会下载基础镜像并复制模型权重。

# 运行 Docker 容器

使用 GPU 支持运行容器：

```bash
docker run --gpus all -p 8501:8501 hichunk:latest
```

您也可以通过覆盖 CMD 来自定义服务器参数：

```bash
docker run --gpus all -p 8501:8501 hichunk:latest \
    --model /app/HiChunk \
    --served-model-name HiChunk \
    --port 8501 \
    --host 0.0.0.0 \
    --trust-remote-code \
    --dtype bfloat16 \
    --max-num-batched-tokens 65536 \
    --enforce-eager \
    --seed 0
```

# 推送到容器注册表

要在远程机器上部署，请将镜像推送到容器注册表：

```bash
# 为您的注册表标记镜像
docker tag hichunk:latest your-registry.com/hichunk:latest

# 推送到注册表
docker push your-registry.com/hichunk:latest
```

# 在远程机器上运行

在远程机器上，拉取并运行镜像：

```bash
# 拉取镜像
docker pull your-registry.com/hichunk:latest

# 运行容器
docker run --gpus all -p 8501:8501 your-registry.com/hichunk:latest
```

HiChunk 服务将在 `http://<remote-machine-ip>:8501` 可用。

# 最终目录结构

构建前，您的 `hichunk-docker` 目录应具有以下结构：

```
hichunk-docker/
├── Dockerfile
├── utu_v1.py
├── registry.py
└── HiChunk/
    ├── config.json
    ├── configuration_utu_v1.py
    ├── generation_config.json
    ├── model-00001-of-00002.safetensors
    ├── model-00002-of-00002.safetensors
    ├── model.safetensors.index.json
    ├── modeling_utu_v1.py
    ├── registry.py
    ├── special_tokens_map.json
    ├── tokenizer_config.json
    ├── tokenizer.json
    ├── trainer_state.json
    └── utu_v1.py
```
