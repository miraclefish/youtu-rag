"""
Answer Generator Module - é€šç”¨ç­”æ¡ˆç”Ÿæˆå™¨
ä»æ‰§è¡Œç»“æœç”Ÿæˆç¬¦åˆè¦æ±‚çš„è‡ªç„¶è¯­è¨€ç­”æ¡ˆ
"""

import pandas as pd
from typing import Any, Dict, Optional
import re
import logging
import os
import sys

# å¯¼å…¥baselineçš„promptæ¨¡æ¿
try:
    # æ·»åŠ benchmarksè·¯å¾„åˆ°sys.path
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(script_dir)))
    benchmarks_path = os.path.join(project_root, 'benchmarks', 'realhitbench', 'eval', 'inference')
    if benchmarks_path not in sys.path:
        sys.path.insert(0, benchmarks_path)
    
    from answer_prompt_llm import Answer_Prompt
    PROMPT_TEMPLATES_AVAILABLE = True
except ImportError as e:
    PROMPT_TEMPLATES_AVAILABLE = False
    Answer_Prompt = {}

# å¯¼å…¥Data Analysisè¯¦ç»†prompts
try:
    from src.config.data_analysis_prompts import (
        RUDIMENTARY_EXPLORATORY_PROMPT,
        SUMMARY_ANALYSIS_PROMPT,
        ANOMALY_ANALYSIS_PROMPT,
        PREDICTIVE_ANALYSIS_PROMPT
    )
    DATA_ANALYSIS_PROMPTS_AVAILABLE = True
except ImportError:
    DATA_ANALYSIS_PROMPTS_AVAILABLE = False
    RUDIMENTARY_EXPLORATORY_PROMPT = ""
    SUMMARY_ANALYSIS_PROMPT = ""
    ANOMALY_ANALYSIS_PROMPT = ""
    PREDICTIVE_ANALYSIS_PROMPT = ""


class AnswerGenerator:
    """é€šç”¨ç­”æ¡ˆç”Ÿæˆå™¨ - å°†DataFrameç»“æœè½¬æ¢ä¸ºç®€æ´çš„è‡ªç„¶è¯­è¨€ç­”æ¡ˆ"""
    
    def __init__(self, llm_client):
        self.llm_client = llm_client
        self.logger = logging.getLogger(__name__)
    
    def generate_answer(
        self,
        user_query: str,
        result_df: pd.DataFrame,
        execution_trace: list = None,
        question_type: str = "",
        sub_q_type: str = "",
        all_path_results: list = None,
        original_df: pd.DataFrame = None,
        enable_reflection: bool = False,  # â­ ç¦ç”¨è¿­ä»£åæ€ï¼ˆæ€§èƒ½ä¸‹é™10.75 pointsï¼‰
        markdown_table: str = None,  # â­ Schemaå¢å¼º: è¿‡æ»¤åçš„markdownè¡¨æ ¼
        schema_result = None  # â­ Schemaå¢å¼º: Schema Linkingç»“æœ
    ) -> str:
        """
        ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼ˆåŸºäºæ‰€æœ‰pathsçš„ç»“æœï¼‰
        
        V8.1å¢å¼º: æ”¯æŒè¿­ä»£åæ€æ¨ç†ï¼ˆIterative Reflectionï¼‰
        - Round 1: ç”Ÿæˆåˆæ­¥ç­”æ¡ˆ
        - Round 2: éªŒè¯å¹¶æ”¹è¿›ç­”æ¡ˆï¼ˆæ£€æŸ¥æ•°å­—ã€æ ¼å¼ã€å®Œæ•´æ€§ï¼‰
        
        Schemaå¢å¼º: ä½¿ç”¨markdown_tableå’Œschema_resultéªŒè¯ç­”æ¡ˆå‡†ç¡®æ€§
        
        Args:
            user_query: ç”¨æˆ·é—®é¢˜
            result_df: æœ€ä½³pathçš„æ‰§è¡Œç»“æœDataFrame
            execution_trace: æ‰§è¡Œè½¨è¿¹
            question_type: é—®é¢˜ç±»å‹
            all_path_results: æ‰€æœ‰pathsçš„ç»“æœï¼ˆç”¨äºç»¼åˆåˆ¤æ–­ï¼‰
            original_df: åŸå§‹è¡¨æ ¼æ•°æ®ï¼ˆå‰100è¡Œï¼‰
            enable_reflection: æ˜¯å¦å¯ç”¨è¿­ä»£åæ€ï¼ˆé»˜è®¤Trueï¼Œå»ºè®®å¯¹çŸ­ç­”æ¡ˆç±»å‹å¯ç”¨ï¼‰
            markdown_table: è¿‡æ»¤åçš„markdownæ ¼å¼è¡¨æ ¼ï¼ˆæ¥è‡ªSchema Linkingï¼‰
            schema_result: Schema Linkingç»“æœï¼ˆåŒ…å«é€‰ä¸­çš„è¡Œåˆ—headersï¼‰
            
        Returns:
            æ ¼å¼åŒ–ç­”æ¡ˆ: "[Final Answer]: <answer>"
        """
        
        # ç©ºç»“æœå¤„ç† - æ”¹è¿›ï¼šå…ˆå°è¯•ä»åŸå§‹æ•°æ®ä¸­æå–ç­”æ¡ˆ
        if result_df is None or result_df.empty:
            # å¦‚æœåŸå§‹æ•°æ®å­˜åœ¨ï¼Œå°è¯•ä»åŸå§‹æ•°æ®ä¸­æå–ç­”æ¡ˆ
            if original_df is not None and not original_df.empty:
                self.logger.warning("Result DataFrame is empty, attempting to extract answer from original data")
                # å°è¯•ä»åŸå§‹æ•°æ®ä¸­æå–ç®€å•ç­”æ¡ˆ
                fallback_answer = self._try_extract_from_original(user_query, original_df, question_type)
                if fallback_answer and "no data" not in fallback_answer.lower():
                    return fallback_answer
            return "[Final Answer]: No data available"
        
        # â­ æ£€æµ‹æ˜¯å¦ä¸ºçŸ­ç­”æ¡ˆç±»å‹ï¼ˆFact Checking, Numerical Reasoning, Structure Comprehendingï¼‰
        is_short_answer_type = self._is_short_answer_type(question_type, sub_q_type)
        
        # ==================== Round 1: åˆæ­¥ç­”æ¡ˆç”Ÿæˆ ====================
        self.logger.info("ğŸ“ Answer Generation: Round 1 - Initial answer...")
        
        # æ„å»ºpromptï¼ˆåŒ…å«æ‰€æœ‰pathsçš„ç»“æœå’ŒåŸå§‹æ•°æ®ï¼‰
        prompt_round1 = self._build_prompt(
            user_query, 
            result_df, 
            question_type,
            sub_q_type=sub_q_type,
            all_path_results=all_path_results,
            original_df=original_df,
            use_concise_mode=is_short_answer_type,  # â­ ä¼ é€’çŸ­ç­”æ¡ˆæ¨¡å¼æ ‡å¿—
            markdown_table=markdown_table,  # â­ Schemaå¢å¼º
            schema_result=schema_result  # â­ Schemaå¢å¼º
        )
        
        try:
            # Round 1: è°ƒç”¨LLMç”Ÿæˆåˆæ­¥ç­”æ¡ˆ
            raw_answer_round1 = self.llm_client.call_api(prompt_round1)
            
            # æ¸…ç†å¹¶æ ¼å¼åŒ–
            formatted_round1 = self._format_answer(raw_answer_round1, is_short_answer=is_short_answer_type)
            
            # å¦‚æœä¸å¯ç”¨åæ€ï¼Œç›´æ¥è¿”å›
            if not enable_reflection:
                self.logger.info("âœ… Answer generated (no reflection)")
                return formatted_round1
            
            # å¯¹äºé•¿ç­”æ¡ˆç±»å‹ï¼ˆData Analysis, Visualizationï¼‰ï¼Œè·³è¿‡åæ€ï¼ˆé¿å…è¿‡åº¦ä¼˜åŒ–ï¼‰
            if not is_short_answer_type:
                self.logger.info("âœ… Answer generated (long-form, skipping reflection)")
                return formatted_round1
            
            # ==================== Round 2: è¿­ä»£åæ€ä¸éªŒè¯ ====================
            self.logger.info("ğŸ”„ Answer Generation: Round 2 - Reflection and verification...")
            
            # æå–çº¯ç­”æ¡ˆï¼ˆç§»é™¤[Final Answer]:æ ‡è®°ï¼‰
            clean_answer_round1 = formatted_round1.replace("[Final Answer]:", "").strip()
            
            prompt_round2 = f"""You previously answered a question. Now verify and improve your answer.

**Original Question**: {user_query}
**Question Type**: {question_type}

**Your Previous Answer**: {clean_answer_round1}

**Execution Result**:
{self._format_dataframe(result_df, max_rows=20, use_markdown=True)}

**Original Table Data (for verification)**:
{self._format_dataframe(original_df.head(50), max_rows=50, use_markdown=True) if original_df is not None else "Not available"}

---

**VERIFICATION CHECKLIST** (â­ CRITICAL - Check each item):

1. **Data Extraction Accuracy**
   - â–¡ Did I extract data from the CORRECT row(s)?
   - â–¡ Did I use the CORRECT column(s)?
   - â–¡ Are the numbers EXACTLY matching the table?
   - â–¡ Did I confuse similar column names (e.g., "Total employed" vs "Employed total")?

2. **Calculation Accuracy** (if applicable)
   - â–¡ Is my arithmetic correct? (Re-calculate: sum, average, max, min)
   - â–¡ Did I include/exclude the right rows? (Check for "Total", "Sum", "Average" rows)
   - â–¡ Did I apply the right formula?

3. **Number Format Consistency**
   - â–¡ Integers: No decimal point (e.g., 1955, 62170, NOT 1955.0)
   - â–¡ Two-decimal numbers: Always .XX format (e.g., 43.60, 0.70, NOT 43.6 or 0.7)
   - â–¡ Remove thousand separators (99826, NOT 99,826)

4. **Answer Completeness**
   - â–¡ Did I answer ALL parts of the question?
   - â–¡ If multiple values requested, did I use comma-separated format? (e.g., "1955, 62170")

5. **Answer Brevity (SHORT ANSWER MODE)**
   - â–¡ Is this a Fact Checking / Numerical Reasoning question? â†’ Output ONLY the answer
   - â–¡ NO explanations like "The answer is...", "Based on the data..."
   - â–¡ NO context like "Men are larger than women", just output "Men"
   - â–¡ Examples:
     * Good: "Men" | Bad: "Men (85500) are larger in number than women (75537)"
     * Good: "1955, 62170" | Bad: "The year with max was 1955 with population 62170"
     * Good: "Yes" | Bad: "Yes, the value is higher"

6. **Edge Cases**
   - â–¡ If answer not found in data, say "Cannot be determined" (NOT "No such year exists")
   - â–¡ For Yes/No questions, answer ONLY "Yes" or "No"

---

**INSTRUCTIONS**:
1. Review your previous answer against the checklist above
2. If you found ANY errors or formatting issues, OUTPUT the CORRECTED answer
3. If your previous answer is perfect, OUTPUT it again unchanged
4. Format: "[Final Answer]: <your verified/corrected answer>"

**IMPORTANT**: Output ONLY the final answer line, no explanations about what you changed!

Output:"""
            
            # Round 2: è°ƒç”¨LLMè¿›è¡Œåæ€å’ŒéªŒè¯
            raw_answer_round2 = self.llm_client.call_api(prompt_round2)
            
            # æ¸…ç†å¹¶æ ¼å¼åŒ–
            formatted_round2 = self._format_answer(raw_answer_round2, is_short_answer=is_short_answer_type)
            
            self.logger.info("âœ… Answer refined through reflection")
            return formatted_round2
            
        except Exception as e:
            self.logger.error(f"Answer generation failed: {e}")
            # Fallback: ç®€å•æ‹¼æ¥DataFrameçš„å€¼
            fallback = self._generate_fallback(result_df)
            return f"[Final Answer]: {fallback}"
    
    def _build_prompt(
        self, 
        user_query: str, 
        result_df: pd.DataFrame, 
        question_type: str,
        sub_q_type: str = "",
        all_path_results: list = None,
        original_df: pd.DataFrame = None,
        use_concise_mode: bool = False,  # â­ æ–°å¢ï¼šçŸ­ç­”æ¡ˆæ¨¡å¼
        markdown_table: str = None,  # â­ Schemaå¢å¼ºï¼šè¿‡æ»¤åçš„markdownè¡¨æ ¼
        schema_result = None  # â­ Schemaå¢å¼ºï¼šSchema Linkingç»“æœ
    ) -> str:
        """æ„å»ºç­”æ¡ˆç”Ÿæˆpromptï¼ˆåŒ…å«æ‰€æœ‰pathsçš„ç»“æœå’ŒåŸå§‹æ•°æ®ï¼‰
        
        Schemaå¢å¼º: ä½¿ç”¨markdown_tableæä¾›æ›´æ¸…æ™°çš„æ•°æ®è§†å›¾ï¼Œ
                    ä½¿ç”¨schema_resultæç¤ºç›¸å…³çš„headers
        """
        
        # DataFrameæ ¼å¼åŒ–ï¼ˆæœ€ä½³pathçš„ç»“æœï¼‰
        df_str = self._format_dataframe(result_df, max_rows=20)
        
        # å°è¯•ä½¿ç”¨baselineçš„promptæ¨¡æ¿
        prompt_template = None
        answer_format = "AnswerName1, AnswerName2..."
        
        if PROMPT_TEMPLATES_AVAILABLE and question_type:
            # æ ¹æ®é—®é¢˜ç±»å‹å’Œå­ç±»å‹é€‰æ‹©promptæ¨¡æ¿
            if question_type == "Data Analysis" and sub_q_type:
                # Data Analysisæ ¹æ®SubQTypeé€‰æ‹©
                prompt_template = Answer_Prompt.get(sub_q_type)
                if not prompt_template:
                    prompt_template = Answer_Prompt.get("Rudimentary Analysis", Answer_Prompt.get("Fact Checking"))
            elif question_type in Answer_Prompt:
                prompt_template = Answer_Prompt.get(question_type)
            
            # è®¾ç½®ç­”æ¡ˆæ ¼å¼
            if sub_q_type == "Exploratory Analysis":
                answer_format = "CorrelationRelation, CorrelationCoefficient"
            elif question_type == "Visualization":
                answer_format = "Python code"
            elif question_type == "Summary Analysis":
                answer_format = "TableSummary"
            elif question_type == "Anomaly Analysis":
                answer_format = "Conclusion"
        
        # æ·»åŠ åŸå§‹æ•°æ®ï¼ˆå‰100è¡Œï¼‰
        original_data_section = ""
        if original_df is not None and not original_df.empty:
            # â­ Schemaå¢å¼ºï¼šä¼˜å…ˆä½¿ç”¨markdown_tableï¼ˆå·²è¿‡æ»¤ç›¸å…³åˆ—ï¼‰
            if markdown_table:
                original_data_section = "\n# Relevant Table Data (filtered by Schema Linking):\n"
                original_data_section += markdown_table
                original_data_section += "\n**NOTE**: This table has been filtered to show only relevant columns/rows based on the query.\n"
            else:
                original_data_section = "\n# Original Table Data (first 100 rows for context):\n"
                original_data_section += self._format_dataframe(original_df.head(100), max_rows=100)
                original_data_section += "\n**NOTE**: Use this to understand the data structure and verify the result. Pay attention to special indicators such as 'Total', 'Sum', 'Average', 'Mean', etc. in row/column headers.\n"
        
        # â­ Schemaå¢å¼ºï¼šæ·»åŠ Schemaä¿¡æ¯æç¤º
        schema_hint = ""
        if schema_result:
            schema_lines = []
            schema_lines.append("\n# Schema Information (Relevant Headers):")
            
            if schema_result.selected_col_headers:
                schema_lines.append(f"**Relevant Columns** ({len(schema_result.selected_col_headers)}): {', '.join(schema_result.selected_col_headers[:10])}")
            
            if schema_result.selected_row_headers:
                schema_lines.append(f"**Relevant Rows** ({len(schema_result.selected_row_headers)}): {', '.join(schema_result.selected_row_headers[:10])}")
            
            schema_lines.append("**Guidance**: Focus on these headers when extracting the answer.\n")
            schema_hint = "\n".join(schema_lines)
        
        # æ·»åŠ æ‰€æœ‰pathsçš„ç»“æœå¯¹æ¯”
        all_paths_section = ""
        if all_path_results and len(all_path_results) > 1:
            all_paths_section = "\n# All Execution Paths Results:\n"
            for idx, path_result in enumerate(all_path_results[:3], 1):  # æœ€å¤šæ˜¾ç¤º3ä¸ª
                path_df = path_result.get('final_df')
                if path_df is not None and not path_df.empty:
                    all_paths_section += f"\nPath {idx} ({path_result.get('path_id', 'Unknown')}):\n"
                    all_paths_section += f"  Success Rate: {path_result.get('success_count', 0)}/{path_result.get('total_ops', 0)}\n"
                    all_paths_section += f"  Reward: {path_result.get('cumulative_reward', 0):.1f}\n"
                    all_paths_section += f"  Result:\n"
                    all_paths_section += self._format_dataframe(path_df, max_rows=3)
                    all_paths_section += "\n"
            
            all_paths_section += "\n**NOTE**: Multiple paths shown for verification only. Output ONE answer based on the Best Path Result.\n"
        
        # å¦‚æœä½¿ç”¨äº†baselineçš„promptæ¨¡æ¿ï¼Œæ„å»ºå®Œæ•´prompt
        if prompt_template:
            # åˆ†æé—®é¢˜æ„å›¾
            question_analysis = self._analyze_question_intent(user_query)
            
            # â­ çŸ­ç­”æ¡ˆç±»å‹ï¼šä½¿ç”¨ç®€æ´æ¨¡å¼Prompt
            if use_concise_mode:
                # å­ç±»å‹ç‰¹æ®Šå¤„ç†
                calculation_guidance = ""
                if sub_q_type in ["Ranking", "Comparison"]:
                    calculation_guidance = """
**RANKING/COMPARISON CRITICAL INSTRUCTIONS:**
1. EXTRACT ALL relevant items with their numeric values from the Execution Result
2. SORT them correctly by the metric (ascending/descending based on question keywords)
   - "highest", "top", "largest", "most" â†’ descending order (largest first)
   - "lowest", "bottom", "smallest", "least" â†’ ascending order (smallest first)
3. COUNT how many items are requested ("top 3", "top 5", "highest 2", etc.)
4. OUTPUT ONLY those top N items in the correct order
5. FORMAT: "Item1, Item2, Item3" (comma-separated, item names ONLY, NO values, NO parentheses)

**CRITICAL EXAMPLE**:
Question: "List the top 3 products by sales (highest to lowest)."
Execution Result: Product A (100), Product B (200), Product C (150), Product D (80)
Step 1: Extract all â†’ [(B, 200), (C, 150), (A, 100), (D, 80)]
Step 2: Already sorted (highest first) â†’ B > C > A > D
Step 3: Take top 3 â†’ B, C, A
[Final Answer]: Product B, Product C, Product A

âš ï¸ COMMON MISTAKES TO AVOID:
- âŒ Including values: "Product B (200), Product C (150)" â†’ âœ… "Product B, Product C"
- âŒ Wrong order: Starting with smallest when question asks for "highest"
- âŒ Wrong count: Outputting 5 items when question asks for "top 3"
- âŒ Including all items: Must limit to requested count!
"""
                elif sub_q_type == "Counting":
                    calculation_guidance = """
**COUNTING SPECIAL INSTRUCTIONS:**
1. Look for a count/total value in the Execution Result
2. If multiple groups exist, sum them if question asks for "total" or "combined"
3. Output ONLY the number (no units, no explanations)
"""
                elif sub_q_type in ["Inference-based Fact Checking", "Multi-hop Fact Checking"]:
                    calculation_guidance = """
**FACT CHECKING SPECIAL INSTRUCTIONS:**
1. For calculation questions: Perform the calculation step-by-step mentally, then output only the result
2. For Yes/No questions: Check the condition carefully (>, >=, <, <=, =)
3. For "find year when X" questions: Scan ALL rows in Original Table Data
4. Output: For Yes/No â†’ "Yes" or "No", For numbers â†’ the value, For year/text â†’ the exact value
"""
                
                prompt = f"""{prompt_template}

# Table
{original_data_section}
{schema_hint}
# Execution Result (after processing):
{df_str}
{all_paths_section}

# Question Analysis
{question_analysis}

{calculation_guidance}

# â­â­â­ CRITICAL: SHORT ANSWER MODE - NO EXPLANATIONS ALLOWED â­â­â­
**IMPORTANT: This is a SHORT ANSWER question. You MUST output ONLY the value, NO steps, NO reasoning.**

**STRICT FORMAT REQUIREMENT:**
```
[Final Answer]: <value>
```

**RULES (MUST FOLLOW):**
1. **Output ONLY the answer value** - No "To find..." or "Follow these steps..."
2. **Extract from Execution Result OR Original Table Data**:
   - Single value â†’ Just the value (e.g., "1955")
   - Multiple values â†’ Comma-separated (e.g., "1955, 62170")
   - Yes/No â†’ "Yes" or "No" only
3. **Number format**: Remove commas (99,826 â†’ 99826), keep 2 decimals if float
4. **Calculation accuracy**: If you need to calculate average/sum/etc., do it precisely
5. **Boundary checks**: For comparisons, distinguish > vs >= carefully
6. **NO explanations** - The answer line should be THE ONLY line you output

**EXAMPLES OF CORRECT OUTPUT:**
- [Final Answer]: 1955
- [Final Answer]: 158772
- [Final Answer]: 1955, 62170
- [Final Answer]: Yes
- [Final Answer]: 35 to 39 years, 40 to 44 years

**EXAMPLES OF WRONG OUTPUT (DO NOT DO THIS):**
- âŒ "To find the answer, first identify... [Final Answer]: 1955"
- âŒ "From the table, the value is 1955. [Final Answer]: 1955"
- âŒ "The answer is 1955 because..."

**NOW OUTPUT ONLY THE ANSWER LINE:**

# Question
{user_query}

Output format: [Final Answer]: {answer_format}
"""
            else:
                # é•¿ç­”æ¡ˆç±»å‹ï¼šä½¿ç”¨å®Œæ•´åˆ†ææ¨¡å¼
                # æ ¹æ®é—®é¢˜ç±»å‹æ·»åŠ ç‰¹æ®ŠæŒ‡å¯¼
                type_specific_guidance = ""
                
                # â­â­â­ é‡è¦ï¼šåªæœ‰Data Analysisç±»å‹éœ€è¦è¯¦å°½åˆ†æï¼Œå…¶ä»–ç±»å‹ç®€æ´è¾“å‡º â­â­â­
                
                if question_type == "Visualization":
                    type_specific_guidance = """
# â­â­â­ VISUALIZATION: OUTPUT EXECUTABLE PYTHON CODE ONLY â­â­â­

**THIS IS A CODE GENERATION TASK - Your output will be executed by Python**

**MANDATORY CODE STRUCTURE:**
```python
import pandas as pd
import matplotlib.pyplot as plt

# 1. Load data
df = pd.read_excel('filename.xlsx')

# 2. Process data (CRITICAL: Ensure correct columns and aggregations)
#    Example: df_plot = df.groupby('Category')['Value'].sum()

# 3. Create visualization
plt.figure(figsize=(10, 6))
# For Pie Chart: plt.pie(values, labels=labels, autopct='%1.1f%%')
# For Bar Chart: plt.bar(x, y)
# For Line Chart: plt.plot(x, y, marker='o')
plt.title('Title Here')
plt.xlabel('X Label')
plt.ylabel('Y Label')
plt.show()
```

**CRITICAL DATA PROCESSING RULES:**
1. **Column Selection**: Use EXACT column names from Original Table Data
2. **Aggregation**: If needed, use groupby().sum() / groupby().mean() / etc.
3. **Filtering**: Remove non-data rows (headers, totals) if present
4. **Data Validation**: Ensure values are numeric (use pd.to_numeric if needed)

**CHART TYPE REQUIREMENTS:**
- **Pie Chart**: Must have values (sizes) and labels
- **Bar Chart**: Must have x-axis categories and y-axis values
- **Line Chart**: Must have x-axis points and y-axis values
- **Scatter Plot**: Must have x and y coordinates

**FORBIDDEN OUTPUTS:**
âŒ "To create a chart, ..." (NO explanations!)
âŒ "Here is the code..." (NO descriptions!)
âŒ "Follow these steps..." (NO instructions!)

**REMEMBER**: The evaluator will extract Y-axis data from your plot. Ensure data is correctly processed!
"""
                elif question_type == "Data Analysis":
                    if sub_q_type in ["Rudimentary Analysis", "Exploratory Analysis"]:
                        type_specific_guidance = RUDIMENTARY_EXPLORATORY_PROMPT if DATA_ANALYSIS_PROMPTS_AVAILABLE else """
# â­ DATA ANALYSIS: PROVIDE COMPREHENSIVE ANALYSIS â­

**YOU MUST PROVIDE DETAILED ANALYSIS, NOT JUST NUMBERS**

Include: Data overview, calculation process, statistical details, insights, and context.
"""
                    elif sub_q_type == "Summary Analysis":
                        type_specific_guidance = SUMMARY_ANALYSIS_PROMPT if DATA_ANALYSIS_PROMPTS_AVAILABLE else """
# â­ SUMMARY ANALYSIS: PROVIDE COMPREHENSIVE TABLE DESCRIPTION â­

Include: Table structure, column descriptions, key insights & trends, statistical highlights.
"""
                    elif sub_q_type == "Anomaly Analysis":
                        type_specific_guidance = ANOMALY_ANALYSIS_PROMPT if DATA_ANALYSIS_PROMPTS_AVAILABLE else """
# â­ ANOMALY ANALYSIS: IDENTIFY AND EXPLAIN ALL ANOMALIES â­

Include: Detection methodology, identified anomalies with details, root cause analysis, patterns.
"""
                    elif sub_q_type == "Predictive Analysis":
                        type_specific_guidance = PREDICTIVE_ANALYSIS_PROMPT if DATA_ANALYSIS_PROMPTS_AVAILABLE else """
# â­ PREDICTIVE ANALYSIS: SHOW PREDICTION METHODOLOGY â­

Include: Historical analysis, prediction methodology, calculation steps, predicted value, validation.
"""
                    else:
                        type_specific_guidance = """
# â­ DATA ANALYSIS: PROVIDE THOROUGH ANALYSIS â­

For any data analysis, include comprehensive explanation with calculations, insights, and context.
"""
                
                prompt = f"""{prompt_template}

# Table
{original_data_section}
{schema_hint}
# Execution Result (after processing):
{df_str}
{all_paths_section}

# Question Analysis
{question_analysis}

{type_specific_guidance}

# Critical Instructions for Answer Extraction:
1. **Read the question CAREFULLY** - Identify what is being asked:
   - Is it asking for a maximum/minimum value? â†’ Extract the max/min value
   - Is it asking for a specific year/date? â†’ Extract that specific value
   - Is it asking for a count? â†’ Extract the count number
   - Is it asking for multiple values? â†’ Extract all requested values
   - Is it a Yes/No question? â†’ Analyze and answer 'Yes' or 'No'
   - Is it asking for visualization? â†’ Output Python code!

2. **Examine the Execution Result carefully**:
   - If result has 1 row: Extract the relevant value(s) from that row
   - If result has multiple rows: 
     * For max/min questions: Find the row with max/min value and extract it
     * For list questions: Extract all relevant values
     * For count questions: Count the rows or extract the count value
   - If result is empty: Check Original Table Data for direct answer

3. **Check Original Table Data for special indicators**:
   - Look for rows labeled "Total", "Sum", "Average", "Mean"
   - Look for hierarchical groupings (age groups, categories)
   - Understand the table structure before extracting values

4. **Verify your answer**:
   - Does it directly answer the question?
   - Are you extracting the correct column(s)?
   - Are you using the right row(s)?
   - For calculations: Are your numbers accurate?

# Critical Format Rules (MUST FOLLOW):
1. The answer MUST start with '[Final Answer]: '

2. **OUTPUT LENGTH DEPENDS ON QUESTION TYPE** (CRITICAL - READ CAREFULLY):
   
   â­ **FOR DATA ANALYSIS QUESTIONS ONLY** (Rudimentary/Exploratory/Summary/Anomaly/Predictive Analysis):
   - Provide COMPREHENSIVE, DETAILED analysis as specified above
   - Include: Data overview, calculation process, statistical details, insights, context
   - Expected length: 300-700 words with multiple sections
   
   âŒ **FOR ALL OTHER QUESTION TYPES** (Fact Checking, Numerical Reasoning, Comparison, Ranking, etc.):
   - Extract ONLY the final answer value
   - NO explanations, NO analysis, NO reasoning steps
   - Expected length: One line - just "[Final Answer]: <value>"
   
3. **Number formatting**:
   - Remove all thousand separators (commas) from numbers (e.g., 99,826 -> 99826)
   - For numerical answers, keep consistent decimal places (2 decimals for floats)
   
4. **Special answer types**:
   - For Yes/No questions: answer ONLY 'Yes' or 'No'
   - For Visualization questions: Output ONLY executable Python code (no descriptions)
   
5. **Answer accuracy**: Use the original format from the table without modifying it

# Question
{user_query}

Emphasize: you need to make sure your final answer is formatted in this way: [Final Answer]: {answer_format}
"""
        else:
            # ä½¿ç”¨é€šç”¨promptï¼ˆfallbackï¼‰
            type_hint = ""
            if question_type:
                type_hint = f"Question Type: {question_type}\n"
                if sub_q_type:
                    type_hint += f"Sub Type: {sub_q_type}\n"
                
                # å¯è§†åŒ–ç±»å‹ç‰¹æ®Šå¤„ç†
                if question_type == "Visualization":
                    type_hint += """
**â­â­â­ CRITICAL for Visualization questions - OUTPUT CODE, NOT DESCRIPTION! â­â­â­**
- You MUST output EXECUTABLE Python code, NOT natural language description
- Use matplotlib or seaborn
- Include necessary imports (import pandas as pd, import matplotlib.pyplot as plt)
- Load data from the Excel file mentioned in the question
- Create the requested visualization
- Include title, labels, and legend
- End with plt.show()
- DO NOT output "To create a chart..." - OUTPUT THE CODE!

Example format:
```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_excel('employment-table02.xlsx')
# ... data processing ...
plt.figure(figsize=(10, 6))
plt.pie(values, labels=labels, autopct='%1.1f%%')
plt.title('...')
plt.show()
```
"""
                elif question_type == "Numerical Reasoning" and sub_q_type == "Ranking":
                    type_hint += """
**IMPORTANT for Ranking questions**:
1. Extract ALL candidate items with their values from the data
2. Sort them by the metric specified in the question
3. Output ONLY the requested number of items (top 2, top 5, etc.)
4. Format: "Item1, Item2, Item3" (comma-separated, item names only, no values)
5. Check the question carefully for "top", "highest", "descending" etc.
"""
            
            # åˆ†æé—®é¢˜æ„å›¾
            question_analysis = self._analyze_question_intent(user_query)
            
            # â­ çŸ­ç­”æ¡ˆç±»å‹ï¼šä½¿ç”¨ç®€æ´æ¨¡å¼
            if use_concise_mode:
                # å­ç±»å‹ç‰¹æ®ŠæŒ‡å¯¼
                special_guidance = ""
                if sub_q_type in ["Ranking", "Comparison"]:
                    special_guidance = """
**RANKING/COMPARISON CRITICAL INSTRUCTIONS:**
1. EXTRACT ALL items with numeric values from data
2. SORT correctly: "highest/top/most" â†’ descending, "lowest/bottom/least" â†’ ascending
3. COUNT requested items: "top 3" = 3 items, "top 5" = 5 items
4. OUTPUT only top N in correct order
5. FORMAT: "Item1, Item2, Item3" (NO values, NO parentheses)

Example:
Q: "Top 3 regions by population (highest first)"
Data: North (100), South (200), East (150), West (80)
Answer: [Final Answer]: South, East, North
"""
                elif sub_q_type == "Counting":
                    special_guidance = """
**COUNTING INSTRUCTIONS:**
1. Look for count/sum in Execution Result or Original Table
2. If multiple groups, sum them if question says "total" or "all"
3. Output ONLY the number (no text)
"""
                elif sub_q_type in ["Inference-based Fact Checking", "Multi-hop Fact Checking"]:
                    special_guidance = """
**FACT CHECKING INSTRUCTIONS:**
1. For calculations: Compute precisely (sum/count for average, etc.)
2. For Yes/No: Check conditions carefully (>, >=, <, <=, =)
3. For "find when X": Scan ALL rows in Original Table Data
4. For boundary checks: 4.52% is NOT greater than 5% (use exact comparison)
"""
                
                prompt = f"""You are answering a SHORT ANSWER data query.

User Question: {user_query}
{type_hint}
{original_data_section}
{schema_hint}
# Execution Result (after processing):
{df_str}
{all_paths_section}

# Question Analysis
{question_analysis}

{special_guidance}

â­â­â­ SHORT ANSWER MODE - CRITICAL â­â­â­
**You MUST output ONLY "[Final Answer]: <value>" with NO explanations.**

**STRICT RULES:**
1. **NO steps, NO reasoning, NO "To find..."** - Output ONLY the answer line
2. Extract from Execution Result OR Original Table Data:
   - Single value â†’ Output value only
   - Multiple values â†’ Comma-separated
   - Yes/No â†’ "Yes" or "No" only
3. **Number Format (CRITICAL for scoring)**:
   - Integers: NO decimal point (e.g., 1955, 62170, NOT 1955.0)
   - Two-decimal numbers: Always .XX format (e.g., 43.60, 0.70, NOT 43.6 or 0.7)
   - Remove commas: 99,826 â†’ 99826
   - Percentages: Two decimals (e.g., 12.50%)
4. Calculation accuracy: If computing average/sum, do it precisely
5. Boundary checks: Distinguish > vs >= carefully
6. **THE ANSWER LINE SHOULD BE YOUR ENTIRE OUTPUT**

**CORRECT OUTPUT:**
[Final Answer]: 1955

**WRONG OUTPUT (DO NOT DO THIS):**
To find the answer... [Final Answer]: 1955

**NOW OUTPUT ONLY THE ANSWER:**
"""
            else:
                # é•¿ç­”æ¡ˆç±»å‹ï¼šä½¿ç”¨å®Œæ•´prompt
                special_guidance = ""
                if question_type == "Visualization":
                    special_guidance = """
**â­ VISUALIZATION: OUTPUT CODE, NOT DESCRIPTION â­**
You MUST output EXECUTABLE Python code!
"""
                elif question_type == "Data Analysis":
                    special_guidance = """
**â­ DATA ANALYSIS: CHECK CALCULATIONS CAREFULLY â­**
- Verify mean, std, correlation calculations
- Check if you're using the correct data rows
- Look for special indicators in Original Table Data
"""
                
                prompt = f"""You are answering a data query based on execution results.

User Question: {user_query}
{type_hint}
{original_data_section}
{schema_hint}
# Execution Result (after processing):
{df_str}
{all_paths_section}

# Question Analysis
{question_analysis}

{special_guidance}

CRITICAL INSTRUCTIONS:
1. **Read the question CAREFULLY** - Identify what is being asked:
   - Maximum/minimum value? â†’ Extract max/min
   - Specific year/date? â†’ Extract that value
   - Count? â†’ Extract count number
   - Multiple values? â†’ Extract all requested
   - Yes/No? â†’ Answer 'Yes' or 'No'
   - Visualization? â†’ Output Python code (NOT description!)

2. **Examine the Execution Result carefully**:
   - 1 row: Extract value(s) from that row
   - Multiple rows: 
     * Max/min: Find row with max/min value
     * List: Extract all relevant values
     * Count: Count rows or extract count
   - Empty: Check Original Table Data

3. **Check Original Table Data**:
   - Special rows: "Total", "Sum", "Average"
   - Hierarchical groupings (age groups, etc.)
   - Understand structure before extracting

4. **Verify your answer**:
   - Does it directly answer the question?
   - Correct column(s)?
   - Right row(s)?
   - Calculations accurate?

5. **Question Type Handling** (CRITICAL):
   
   â­ **FOR DATA ANALYSIS QUESTIONS ONLY**:
   - Provide comprehensive, detailed analysis report (300-700 words)
   - Include: Data overview, calculation steps, statistical details, insights, context
   
   âŒ **FOR ALL OTHER TYPES** (Fact Checking, Numerical Reasoning, Ranking, etc.):
   - Extract ONLY the specific answer value
   - NO explanations, NO analysis, NO reasoning steps
   
   ğŸ“Š **FOR VISUALIZATION QUESTIONS**:
   - Output ONLY executable Python code (NOT descriptions)

6. **Use Original Table Data for context** - understand meanings and structure

7. **Output ONE answer only** - extract specific information requested

8. **Handle edge cases**:
   - Empty/NaN â†’ "[Final Answer]: No data available"
   - One row â†’ extract value(s)
   - Multiple rows â†’ follow question intent

9. **Format: "[Final Answer]: <answer>"**

10. **For Visualization**: Output code in markdown code block

11. **For multiple values**: comma-space separator (e.g., "1955, 62170")

12. **For single values**: just the value (e.g., "42")

13. **For decimals**: keep 2 decimal places (e.g., 0.44)

14. **For Yes/No**: answer ONLY 'Yes' or 'No'

15. **Remove thousand separators** (99,826 -> 99826)

16. **NEVER include NaN/null** unless truly no data

Examples:
- Q: "What year had max agriculture?" Result: Year=1955 â†’ "[Final Answer]: 1955"
- Q: "Year and population when agriculture was highest?" Result: Year=1955, Pop=62170 â†’ "[Final Answer]: 1955, 62170"
- Q: "Create a bar chart" (Visualization) â†’ "[Final Answer]: ```python\nimport matplotlib.pyplot as plt\n...\nplt.show()```"

Now answer based on the result(s) above. Output ONLY the answer line:
"""
        return prompt
    
    def _format_dataframe(self, df: pd.DataFrame, max_rows: int = 10, use_markdown: bool = True) -> str:
        """
        æ ¼å¼åŒ–DataFrameä¸ºæ–‡æœ¬/Markdownï¼Œä½¿å…¶æ›´å®¹æ˜“ç†è§£
        
        Args:
            df: DataFrame to format
            max_rows: Maximum rows to display
            use_markdown: If True, use Markdown table format (better for LLM understanding)
        """
        
        if df.empty:
            return "Empty DataFrame (no rows)"
        
        # é™åˆ¶æ˜¾ç¤ºè¡Œæ•°
        display_df = df.head(max_rows) if len(df) > max_rows else df
        
        lines = []
        lines.append(f"Shape: {df.shape[0]} rows Ã— {df.shape[1]} columns")
        lines.append(f"Columns: {list(df.columns)}")
        
        # æ·»åŠ æ•°æ®ç±»å‹ä¿¡æ¯
        if len(df.columns) <= 10:  # åªåœ¨åˆ—æ•°ä¸å¤šæ—¶æ˜¾ç¤ºç±»å‹
            dtype_info = {col: str(dtype) for col, dtype in df.dtypes.items()}
            lines.append(f"Data Types: {dtype_info}")
        
        lines.append("")
        lines.append("Data:")
        
        # â­ ä½¿ç”¨Markdownæ ¼å¼ï¼ˆæ›´ç»“æ„åŒ–ï¼ŒLLMæ›´å®¹æ˜“è§£æï¼‰
        if use_markdown:
            try:
                # æ‰‹åŠ¨ç”ŸæˆMarkdownè¡¨æ ¼ï¼ˆå…¼å®¹æ—§ç‰ˆpandasï¼‰
                markdown_table = self._dataframe_to_markdown(display_df)
                lines.append(markdown_table)
            except Exception as e:
                # Fallback to to_string if markdown generation fails
                self.logger.warning(f"Failed to generate markdown table: {e}, falling back to to_string")
                lines.append(display_df.to_string(index=False))
        else:
            lines.append(display_df.to_string(index=False))
        
        # å¦‚æœåªæœ‰å°‘é‡è¡Œï¼Œæ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        if len(df) <= 5:
            lines.append("\nSummary:")
            for col in df.columns:
                if pd.api.types.is_numeric_dtype(df[col]):
                    lines.append(f"  {col}: min={df[col].min()}, max={df[col].max()}, mean={df[col].mean():.2f}")
        
        if len(df) > max_rows:
            lines.append(f"\n(Showing first {max_rows} of {len(df)} rows)")
        
        return "\n".join(lines)
    
    def _dataframe_to_markdown(self, df: pd.DataFrame) -> str:
        """
        æ‰‹åŠ¨å°†DataFrameè½¬æ¢ä¸ºMarkdownè¡¨æ ¼æ ¼å¼
        å…¼å®¹æ—§ç‰ˆpandasï¼ˆ0.23.xï¼‰
        
        Example output:
        | Year | Population | Agriculture |
        |------|------------|-------------|
        | 1953 | 61179      | 15888       |
        | 1955 | 62170      | 16234       |
        """
        if df.empty:
            return "Empty DataFrame"
        
        lines = []
        
        # Header row
        headers = list(df.columns)
        header_line = "| " + " | ".join(str(h) for h in headers) + " |"
        lines.append(header_line)
        
        # Separator row
        separator_line = "|" + "|".join("-" * (len(str(h)) + 2) for h in headers) + "|"
        lines.append(separator_line)
        
        # Data rows
        for idx in range(len(df)):
            row_values = []
            for col in df.columns:
                val = df.iloc[idx][col]
                # Format value
                if pd.isna(val):
                    row_values.append("NaN")
                elif isinstance(val, (int, float)):
                    # Format numbers nicely
                    if isinstance(val, float) and val == int(val):
                        row_values.append(str(int(val)))
                    else:
                        row_values.append(str(val))
                else:
                    row_values.append(str(val))
            
            data_line = "| " + " | ".join(row_values) + " |"
            lines.append(data_line)
        
        return "\n".join(lines)
    
    def _format_answer(self, raw_answer: str, is_short_answer: bool = False) -> str:
        """æ¸…ç†LLMè¾“å‡º
        
        Args:
            raw_answer: LLMåŸå§‹è¾“å‡º
            is_short_answer: æ˜¯å¦ä¸ºçŸ­ç­”æ¡ˆç±»å‹ï¼ˆç”¨äºæ›´æ¿€è¿›çš„æ¸…ç†ï¼‰
        """
        
        answer = raw_answer.strip()
        
        # ğŸ”§ CRITICAL FIX: Clean f-string placeholders from the answer
        # Pattern: {variable_name:.2f} or {object.attr[0]:.1%} etc.
        import re
        fstring_pattern = r'\{[a-zA-Z_][\w]*(?:\.[a-zA-Z_][\w]*)*(?:\[[^\]]+\])*(?::[^}]+)?\}'
        
        # Check if answer contains f-string placeholders
        if re.search(fstring_pattern, answer):
            self.logger.warning("âš ï¸  Found f-string placeholders in answer, cleaning...")
            # Replace with placeholder text
            answer = re.sub(fstring_pattern, '[value]', answer)
            self.logger.warning(f"âœ“ Cleaned f-string placeholders")
        
        # æå– [Final Answer]: åçš„å†…å®¹
        match = re.search(r'\[Final Answer\]:\s*(.+)', answer, re.IGNORECASE | re.DOTALL)
        if match:
            content = match.group(1).strip()
            # ç§»é™¤å¯èƒ½çš„å°¾éƒ¨å¥å·æˆ–æ¢è¡Œ
            content = content.rstrip('.\n')
            
            # â­â­ çŸ­ç­”æ¡ˆç±»å‹ï¼šæ¿€è¿›æ¸…ç†ç­–ç•¥
            if is_short_answer:
                # 1. å¦‚æœç¬¬ä¸€è¡Œçœ‹èµ·æ¥æ˜¯ç­”æ¡ˆï¼ˆçŸ­ä¸”æ²¡æœ‰"To"/"Follow"ç­‰ï¼‰ï¼Œåªå–ç¬¬ä¸€è¡Œ
                first_line = content.split('\n')[0].strip()
                if first_line:
                    # æ£€æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦ä¸ºçº¯å€¼ï¼ˆæ²¡æœ‰è§£é‡Šæ€§æ–‡å­—ï¼‰
                    if len(first_line) < 150 and not any(word in first_line.lower() for word in ['to find', 'follow', 'from the', 'first', 'next', 'then', 'finally']):
                        content = first_line
                        self.logger.info(f"âœ‚ï¸  Extracted first line as short answer: {content[:50]}...")
                    elif len(content) > 200:
                        # å¦‚æœè¶…è¿‡200å­—ç¬¦ä½†ç¬¬ä¸€è¡Œæœ‰è§£é‡Šæ€§æ–‡å­—ï¼Œå°è¯•æ‰¾åˆ°çº¯å€¼
                        # å°è¯•åŒ¹é…å¸¸è§çš„çŸ­ç­”æ¡ˆæ¨¡å¼
                        patterns = [
                            r'^([Yy]es|[Nn]o)\s*$',  # Yes/No
                            r'^(\d+(?:,\d+)*(?:\.\d+)?(?:\s*,\s*\d+(?:,\d+)*(?:\.\d+)?)*)\s*$',  # æ•°å­—
                            r'^([A-Za-z0-9\s,]+(?:\s+years?)?)\s*$',  # å¹´ä»½ã€å¹´é¾„ç»„ç­‰
                        ]
                        for pattern in patterns:
                            value_match = re.match(pattern, first_line)
                            if value_match:
                                content = value_match.group(1).strip()
                                self.logger.info(f"âœ‚ï¸  Extracted value pattern: {content}")
                                break
                        else:
                            # æ²¡æœ‰åŒ¹é…åˆ°æ¨¡å¼ï¼Œå–ç¬¬ä¸€è¡Œ
                            content = first_line
            elif len(content) > 200:
                # éçŸ­ç­”æ¡ˆç±»å‹ï¼šå¦‚æœç­”æ¡ˆè¶…è¿‡200å­—ç¬¦ï¼Œæå–ç¬¬ä¸€è¡Œï¼ˆä¿ç•™åŸé€»è¾‘ï¼‰
                first_line = content.split('\n')[0].strip()
                if first_line and len(first_line) < 200:
                    self.logger.info(f"âœ‚ï¸  Truncated long answer ({len(content)} chars) to first line ({len(first_line)} chars)")
                    content = first_line
            
            # â­â­ æ•°å€¼æ ¼å¼åŒ–ï¼šç§»é™¤ä¸å¿…è¦çš„ .0 åç¼€
            # ä¾‹å¦‚: "158772.0" â†’ "158772", "1955, 62170.0" â†’ "1955, 62170"
            content = self._clean_number_format(content)
            
            return f"[Final Answer]: {content}"
        
        # å¦‚æœæ²¡æœ‰æ ‡è®°ï¼Œæ·»åŠ 
        return f"[Final Answer]: {answer}"
    
    def _generate_fallback(self, df: pd.DataFrame) -> str:
        """Fallbackç­”æ¡ˆï¼ˆLLMå¤±è´¥æ—¶ï¼‰"""
        import re
        
        if df.empty:
            return "No data"
        
        # ğŸ”§ FIX: Clean f-string placeholders from DataFrame
        fstring_pattern = r'\{[a-zA-Z_][\w]*(?:\.[a-zA-Z_][\w]*)*(?:\[[^\]]+\])*(?::[^}]+)?\}'
        
        # å¦‚æœåªæœ‰1è¡Œï¼Œæ‹¼æ¥æ‰€æœ‰éNaNå€¼
        if len(df) == 1:
            values = []
            for col in df.columns:
                val = df.iloc[0][col]
                if pd.notna(val):
                    val_str = str(val)
                    
                    # Skip f-string placeholders
                    if re.match(fstring_pattern, val_str):
                        self.logger.warning(f"Skipping f-string placeholder in fallback: {val_str}")
                        continue
                    
                    # æ ¼å¼åŒ–æ•°å­—
                    if isinstance(val, (int, float)):
                        if val == int(val):
                            values.append(str(int(val)))
                        else:
                            values.append(str(val))
                    else:
                        values.append(val_str)
            
            return ", ".join(values) if values else "No valid data"
        
        # å¤šè¡Œï¼šæ£€æŸ¥æ˜¯å¦æ‰€æœ‰å€¼éƒ½æ˜¯f-string placeholders
        contains_placeholders = False
        for col in df.columns:
            for val in df[col]:
                if pd.notna(val) and re.match(fstring_pattern, str(val)):
                    contains_placeholders = True
                    break
            if contains_placeholders:
                break
        
        if contains_placeholders:
            return f"{len(df)} rows (contains format placeholders - execution may have failed)"
        
        # å¤šè¡Œï¼šè¿”å›ç®€å•æè¿°
        return f"{len(df)} rows found"
    
    def _analyze_question_intent(self, user_query: str) -> str:
        """åˆ†æé—®é¢˜æ„å›¾ï¼Œå¸®åŠ©LLMæ›´å¥½åœ°ç†è§£é—®é¢˜"""
        query_lower = user_query.lower()
        
        intent_parts = []
        
        # æ£€æµ‹é—®é¢˜ç±»å‹
        if any(word in query_lower for word in ['max', 'maximum', 'highest', 'largest', 'greatest', 'top']):
            intent_parts.append("- **Intent**: Finding MAXIMUM value")
            intent_parts.append("- **Action**: Look for the row with the highest value in the relevant column")
        elif any(word in query_lower for word in ['min', 'minimum', 'lowest', 'smallest', 'bottom']):
            intent_parts.append("- **Intent**: Finding MINIMUM value")
            intent_parts.append("- **Action**: Look for the row with the lowest value in the relevant column")
        elif any(word in query_lower for word in ['count', 'number of', 'how many']):
            intent_parts.append("- **Intent**: Counting items")
            intent_parts.append("- **Action**: Count rows or extract count value")
        elif any(word in query_lower for word in ['yes', 'no', 'is', 'are', 'does', 'do', 'can', 'will']):
            if '?' in user_query:
                intent_parts.append("- **Intent**: Yes/No question")
                intent_parts.append("- **Action**: Analyze data and answer 'Yes' or 'No'")
        elif any(word in query_lower for word in ['list', 'all', 'every', 'each']):
            intent_parts.append("- **Intent**: Listing multiple values")
            intent_parts.append("- **Action**: Extract all relevant values")
        elif any(word in query_lower for word in ['when', 'year', 'date', 'time']):
            intent_parts.append("- **Intent**: Finding specific time/date")
            intent_parts.append("- **Action**: Extract the year/date value")
        elif any(word in query_lower for word in ['what', 'which', 'who', 'where']):
            intent_parts.append("- **Intent**: Finding specific entity/value")
            intent_parts.append("- **Action**: Extract the specific value(s) requested")
        
        if not intent_parts:
            intent_parts.append("- **Intent**: General query")
            intent_parts.append("- **Action**: Extract the relevant information from the result")
        
        return "\n".join(intent_parts)
    
    def _generate_direct_llm_answer(
        self, 
        user_query: str, 
        original_df: pd.DataFrame, 
        question_type: str,
        sub_q_type: str = "",
        enable_multi_round: bool = True
    ) -> str:
        """
        ç›´æ¥ä½¿ç”¨LLMå›ç­”ï¼Œä¸ä¾èµ–ä»£ç æ‰§è¡Œç»“æœï¼ˆä½œä¸ºé”™è¯¯å…œåº•ï¼‰
        ç±»ä¼¼äºbaselineæ–¹æ³•ï¼Œç›´æ¥è®©LLMçœ‹è¡¨æ ¼æ•°æ®å›ç­”
        
        V8.1å¢å¼º: æ”¯æŒmulti-round reasoningå’Œreflection
        RealtHitBenchå¢å¼º: çŸ­ç­”æ¡ˆç±»å‹ä½¿ç”¨ç®€æ´Prompt
        """
        try:
            # â­ åˆ¤æ–­æ˜¯å¦ä¸ºçŸ­ç­”æ¡ˆç±»å‹
            is_short_answer = self._is_short_answer_type(question_type, sub_q_type)
            
            # è·å–promptæ¨¡æ¿
            prompt_template = None
            answer_format = "AnswerName1, AnswerName2..."
            
            if PROMPT_TEMPLATES_AVAILABLE and question_type:
                if question_type == "Data Analysis" and sub_q_type:
                    prompt_template = Answer_Prompt.get(sub_q_type)
                    if not prompt_template:
                        prompt_template = Answer_Prompt.get("Rudimentary Analysis", Answer_Prompt.get("Fact Checking"))
                elif question_type in Answer_Prompt:
                    prompt_template = Answer_Prompt.get(question_type)
                
                if sub_q_type == "Exploratory Analysis":
                    answer_format = "CorrelationRelation, CorrelationCoefficient"
                elif question_type == "Visualization":
                    answer_format = "Python code"
                elif question_type == "Summary Analysis":
                    answer_format = "TableSummary"
                elif question_type == "Anomaly Analysis":
                    answer_format = "Conclusion"
            
            # æ ¼å¼åŒ–è¡¨æ ¼æ•°æ®ï¼ˆæ˜¾ç¤ºæ›´å¤šè¡Œï¼Œè®©LLMæœ‰è¶³å¤Ÿä¿¡æ¯ï¼‰
            table_str = self._format_dataframe(original_df, max_rows=200)
            
            # ==================== Round 1: Initial Generation ====================
            self.logger.info(f"ğŸ”„ Fallback LLM: Round 1 - Initial answer generation... (Short Answer Mode: {is_short_answer})")
            
            # æ„å»ºpromptï¼ˆåŒºåˆ†çŸ­ç­”æ¡ˆå’Œé•¿ç­”æ¡ˆï¼‰
            if is_short_answer:
                # â­ çŸ­ç­”æ¡ˆï¼šç®€æ´Prompt
                if prompt_template:
                    prompt = f"""{prompt_template}

# Table
{table_str}

# Question
{user_query}

â­â­â­ CONCISE ANSWER MODE - NO EXPLANATIONS â­â­â­
**This question requires a SHORT answer. You MUST output ONLY the value.**

**CRITICAL - DO NOT include any of these:**
- âŒ "To find..." or "Follow these steps..."
- âŒ "From the table..."
- âŒ Any reasoning or explanation

**ONLY OUTPUT:**
```
[Final Answer]: <value>
```

**Instructions:**
1. Extract the EXACT answer from the table
2. Single value â†’ Output value only (e.g., "1955")
3. Multiple values â†’ Comma-separated (e.g., "1955, 62170")
4. Yes/No â†’ "Yes" or "No" only
5. Remove commas from numbers: 99,826 â†’ 99826
6. **NEVER** use {{placeholders}}

**Example - CORRECT:**
[Final Answer]: 1955

**Example - WRONG:**
To find the year, look at row 5. [Final Answer]: 1955

**NOW OUTPUT ONLY THE ANSWER LINE:**
"""
                else:
                    prompt = f"""Answer this question based on table data.

# Table
{table_str}

# Question
{user_query}

â­â­â­ CONCISE ANSWER MODE - Critical â­â­â­
**This is a SHORT ANSWER question. Output ONLY the value, NO explanations.**

**STRICT RULES - DO NOT:**
- Include "To find..." or step-by-step reasoning
- Add "From the table..." or explanations
- Use {{placeholders}}

**ONLY OUTPUT:**
[Final Answer]: <value>

**Format rules:**
- Single value â†’ e.g., "1955"
- Multiple values â†’ e.g., "1955, 62170"  
- Yes/No â†’ "Yes" or "No" only
- Remove commas from numbers

**Example: If asking for year 1955, ONLY output:**
[Final Answer]: 1955

**Now output your answer:**
"""
            else:
                # é•¿ç­”æ¡ˆï¼šå®Œæ•´Promptï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                if prompt_template:
                    prompt = f"""{prompt_template}

# Table
{table_str}

# Question
{user_query}

Emphasize: you need to make sure your final answer is formatted in this way: [Final Answer]: {answer_format}

# Critical Instructions:
1. Read the table carefully and understand its structure
2. Pay attention to special indicators such as 'Total', 'Sum', 'Average', 'Mean', etc. in row/column headers
3. Identify the relevant rows and columns based on the question
4. Extract the exact answer from the table
5. For numerical answers, keep 2 decimal places for floats
6. Remove thousand separators from numbers
7. For Yes/No questions, answer ONLY 'Yes' or 'No'
8. Use the original format from the table when possible
9. **NEVER** use format placeholders like {{{{var:.2f}}}} - use actual numbers

Now provide your answer:
"""
                else:
                    # ä½¿ç”¨é€šç”¨prompt
                    prompt = f"""You are answering a question based on table data.

# Table
{table_str}

# Question
{user_query}

# Question Type: {question_type}

# Instructions:
1. Read the table carefully
2. Pay attention to special indicators such as 'Total', 'Sum', 'Average', 'Mean', etc.
3. Identify relevant rows and columns
4. Extract the exact answer
5. Format: "[Final Answer]: <answer>"
6. For numerical answers, keep 2 decimal places for floats
7. Remove thousand separators from numbers
8. For Yes/No questions, answer ONLY 'Yes' or 'No'
9. **NEVER** use format placeholders like {{{{var:.2f}}}} - use actual numbers

Now provide your answer:
"""
            
            raw_answer = self.llm_client.call_api(prompt)
            first_formatted = self._format_answer(raw_answer, is_short_answer=is_short_answer)
            
            # å¦‚æœä¸å¯ç”¨multi-roundï¼Œç›´æ¥è¿”å›
            if not enable_multi_round:
                return first_formatted
            
            # ==================== Round 2: Deep Reflection & Refinement ====================
            self.logger.info("ğŸ”„ Fallback LLM: Round 2 - Deep reflection and refinement...")
            
            # æ£€æµ‹ç¬¬ä¸€è½®ç­”æ¡ˆçš„é—®é¢˜
            issues = []
            if self._has_format_errors(first_formatted):
                issues.append("âŒ Contains format placeholders like {{var:.2f}}")
            if len(first_formatted) < 50:
                issues.append("âš ï¸  Answer is too brief (< 50 chars)")
            if "unable to generate" in first_formatted.lower() or "technical error" in first_formatted.lower():
                issues.append("âš ï¸  Contains error messages")
            
            issues_text = "\n".join(f"  {issue}" for issue in issues) if issues else "  âœ… No major issues detected"
            
            critique_prompt = f"""# ğŸ” CRITICAL SELF-REVIEW (Fallback Mode - Code Execution Failed)

You previously generated this answer when the system couldn't execute code:

## Your First Draft
{first_formatted}

---

## Original Question
{user_query}

---

## Automatic Quality Check
{issues_text}

---

## ğŸ“Š Available Data (for verification)
{table_str}

---

## ğŸ“‹ Deep Reflection Checklist

**1. Format & Structure (CRITICAL!)**
- [ ] Are there any {{{{placeholder}}}} or {{{{var:.2f}}}} format strings? **MUST REMOVE!**
- [ ] Is the answer in proper Markdown format (##, |, **, *)?
- [ ] Does it start with "[Final Answer]:" if required?

**2. Accuracy & Data Extraction**
- [ ] Did I extract **exact** numbers from the table?
- [ ] Did I verify values against the data above?
- [ ] Are column names and row values correct?

**3. Completeness**
- [ ] Did I answer **ALL parts** of the question?
- [ ] Are there any error messages or "unable to" statements?
- [ ] Is the answer complete and not truncated?

**4. Analysis Quality (for Data Analysis questions)**
- [ ] Did I explain **WHY**, not just WHAT?
- [ ] Did I identify patterns or trends?
- [ ] Did I provide context or implications?

**5. Common Errors to Fix**
- âŒ {{{{var:.2f}}}} placeholders â†’ Replace with actual numbers
- âŒ "Unable to generate" â†’ Provide actual analysis
- âŒ Generic statements â†’ Add specific data points
- âŒ Missing data â†’ Extract from table above

---

## ğŸ¯ Your Task

Generate an **IMPROVED VERSION** that:
1. **Removes ALL format placeholders** ({{{{...}}}}) - this is NON-NEGOTIABLE!
2. **Extracts exact data** from the table above
3. **Provides complete analysis** (no error messages)
4. **Uses proper format** (Markdown for analysis, "[Final Answer]: value" for simple questions)

**If the first answer is already good** (no placeholders, complete, accurate), output it with minor improvements.

---

Output your **REFINED ANSWER** now:
"""
            
            refined_answer = self.llm_client.call_api(critique_prompt)
            refined_formatted = self._format_answer(refined_answer, is_short_answer=is_short_answer)
            
            self.logger.info("âœ… Fallback LLM: Multi-round completed (2 rounds)")
            return refined_formatted
            
        except Exception as e:
            self.logger.error(f"Failed to generate direct LLM answer: {e}")
            return "[Final Answer]: No data available"
    
    def _has_format_errors(self, answer: str) -> bool:
        """æ£€æµ‹ç­”æ¡ˆæ˜¯å¦åŒ…å«æ ¼å¼é”™è¯¯ï¼ˆå¦‚format placeholdersï¼‰"""
        import re
        # æ£€æµ‹ {xxx:xxx} æ ¼å¼çš„å ä½ç¬¦
        if re.search(r'\{[^}]*:[^}]*\}', answer):
            return True
        # æ£€æµ‹ {xxx()} æ ¼å¼çš„å‡½æ•°è°ƒç”¨å ä½ç¬¦
        if re.search(r'\{\w+\([^)]*\)[^}]*\}', answer):
            return True
        return False
    
    def _try_extract_from_original(self, user_query: str, original_df: pd.DataFrame, question_type: str) -> Optional[str]:
        """
        å°è¯•ä»åŸå§‹æ•°æ®ä¸­æå–ç­”æ¡ˆï¼ˆå½“æ‰§è¡Œç»“æœä¸ºç©ºæ—¶ï¼‰
        
        Args:
            user_query: ç”¨æˆ·é—®é¢˜
            original_df: åŸå§‹æ•°æ®DataFrame
            question_type: é—®é¢˜ç±»å‹
            
        Returns:
            æå–çš„ç­”æ¡ˆï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å›None
        """
        try:
            # æ„å»ºä¸€ä¸ªç®€å•çš„promptï¼Œè®©LLMä»åŸå§‹æ•°æ®ä¸­æå–ç­”æ¡ˆ
            prompt = f"""You need to answer a question based on the original table data.

Question: {user_query}
Question Type: {question_type}

# Original Table Data:
{self._format_dataframe(original_df, max_rows=50)}

IMPORTANT: Even if the data seems incomplete, try to extract the best possible answer from the available data.
- Look for keywords in the question that match column names or row values
- For numerical questions, try to find the relevant numbers
- For yes/no questions, analyze the data and answer 'Yes' or 'No'
- Only return "[Final Answer]: No data available" if absolutely no relevant information exists

Output format: [Final Answer]: <your answer>
"""
            
            raw_answer = self.llm_client.call_api(prompt)
            formatted = self._format_answer(raw_answer)
            
            # æ£€æŸ¥æ˜¯å¦çœŸçš„æ²¡æœ‰æ•°æ®
            if "no data available" in formatted.lower() or "no data" in formatted.lower():
                return None
            
            return formatted
            
        except Exception as e:
            self.logger.warning(f"Failed to extract from original data: {e}")
            return None
    
    def _clean_number_format(self, text: str) -> str:
        """
        æ¸…ç†æ•°å­—æ ¼å¼ï¼šç§»é™¤ä¸å¿…è¦çš„.0åç¼€
        ä¾‹å¦‚: "158772.0" â†’ "158772", "1955, 62170.0" â†’ "1955, 62170"
        ä½†ä¿ç•™çœŸæ­£çš„å°æ•°: "5.80" â†’ "5.80"
        """
        import re
        
        # åŒ¹é…æ•°å­—.0çš„æ¨¡å¼ï¼ˆä½†ä¸æ˜¯0.xè¿™ç§çœŸæ­£çš„å°æ•°ï¼‰
        # ç­–ç•¥ï¼šæ›¿æ¢æ‰€æœ‰ æ•°å­—.0 ä¸º æ•°å­—ï¼ˆåªè¦ä¸æ˜¯ 0.x è¿™ç§å°äº1çš„å°æ•°ï¼‰
        def replace_fn(match):
            full_num = match.group(0)
            # æ£€æŸ¥æ˜¯å¦ä»¥ .0 ç»“å°¾ä¸”å‰é¢çš„æ•°å­—å¤§äºç­‰äº1
            if '.' in full_num:
                parts = full_num.split('.')
                if len(parts) == 2 and parts[1] == '0':
                    # å¦‚æœæ•´æ•°éƒ¨åˆ†æ˜¯0ï¼Œä¿ç•™ï¼ˆå¦‚0.0ï¼‰
                    # å¦åˆ™ç§»é™¤.0
                    try:
                        int_part = int(parts[0])
                        if int_part >= 1:
                            return parts[0]
                    except:
                        pass
            return full_num
        
        # åŒ¹é…æ‰€æœ‰æ•°å­—ï¼ˆåŒ…æ‹¬å°æ•°ï¼‰
        cleaned = re.sub(r'\d+\.?\d*', replace_fn, text)
        return cleaned
    
    def _is_short_answer_type(self, question_type: str, sub_q_type: str = "") -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºçŸ­ç­”æ¡ˆç±»å‹ï¼ˆéœ€è¦ç®€æ´è¾“å‡ºï¼‰
        
        çŸ­ç­”æ¡ˆç±»å‹åŒ…æ‹¬:
        - Fact Checking (78.6% of RealtHitBench)
        - Numerical Reasoning
        - Structure Comprehending
        
        é•¿ç­”æ¡ˆç±»å‹åŒ…æ‹¬:
        - Data Analysis (except Rudimentary Analysis)
        - Visualization
        
        Returns:
            True if short answer type, False otherwise
        """
        # æ˜ç¡®çš„é•¿ç­”æ¡ˆç±»å‹
        if question_type == "Visualization":
            return False
        
        if question_type == "Data Analysis":
            # Data Analysisä¸­ï¼Œåªæœ‰Rudimentary Analysisæ˜¯çŸ­ç­”æ¡ˆ
            if sub_q_type == "Rudimentary Analysis":
                return True
            # Summary/Exploratory/Predictive/Anomaly Analysis éœ€è¦é•¿ç­”æ¡ˆ
            return False
        
        # æ‰€æœ‰å…¶ä»–ç±»å‹é»˜è®¤ä¸ºçŸ­ç­”æ¡ˆ
        # Fact Checking, Numerical Reasoning, Structure Comprehending
        return True

