"""
DTR Framework - Core Data Structures
All structures are serializable and inspectable for debugging
"""

from typing import List, Dict, Any, Optional, Set
from dataclasses import dataclass, field
from enum import Enum
import json


# ==================== Operator Structures ====================

class OperatorType(Enum):
    """Operator categories"""
    DATA_UNDERSTANDING = "data_understanding"
    DATA_CLEANING = "data_cleaning"
    FILTERING_EXTRACTION = "filtering_extraction"
    TRANSFORMATION = "transformation"
    AGGREGATION = "aggregation"
    MULTI_TABLE = "multi_table"
    VALIDATION = "validation"


@dataclass
class Operator:
    """
    Single operator node extracted by ADO.
    Does NOT contain execution order or path information.
    """
    name: str
    category: OperatorType
    description: str
    required_columns: List[str] = field(default_factory=list)
    produced_columns: List[str] = field(default_factory=list)
    semantic_description: str = ""
    estimated_cost: float = 1.0  # For MCTS heuristics
    
    def to_dict(self) -> Dict[str, Any]:
        """Serialize to dict for logging"""
        return {
            "name": self.name,
            "category": self.category.value,
            "description": self.description,
            "required_columns": self.required_columns,
            "produced_columns": self.produced_columns,
            "semantic_description": self.semantic_description,
            "estimated_cost": self.estimated_cost
        }
    
    def __repr__(self):
        return f"Operator({self.name}, cat={self.category.value}, cost={self.estimated_cost})"


# ==================== Path Structures ====================

@dataclass
class ExecutionPath:
    """
    Ordered sequence of operators forming an execution plan.
    Generated by MCTS Planner.
    """
    operators: List[str]  # Operator names in execution order
    estimated_reward: float = 0.0  # Heuristic reward from MCTS
    structural_score: float = 0.0  # Dependency satisfaction score
    path_id: str = ""
    reasoning: str = ""  # Why this path was selected
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "path_id": self.path_id,
            "operators": self.operators,
            "estimated_reward": self.estimated_reward,
            "structural_score": self.structural_score,
            "reasoning": self.reasoning
        }
    
    def __repr__(self):
        return f"Path({self.path_id}: {' → '.join(self.operators)}, R={self.estimated_reward:.2f})"


# ==================== Table State ====================

@dataclass
class TableState:
    """
    Represents the current state of the dataframe during execution.
    Tracks schema changes, shape changes, and available columns.
    """
    columns: List[str] = field(default_factory=list)
    column_types: Dict[str, str] = field(default_factory=dict)
    row_count: int = 0
    shape: tuple = (0, 0)
    available_columns: Set[str] = field(default_factory=set)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "columns": self.columns,
            "column_types": self.column_types,
            "row_count": self.row_count,
            "shape": list(self.shape),
            "available_columns": list(self.available_columns)
        }
    
    def update_from_dataframe(self, df):
        """Update state from pandas DataFrame"""
        import pandas as pd
        if df is not None and isinstance(df, pd.DataFrame):
            self.columns = list(df.columns)
            self.column_types = {col: str(dtype) for col, dtype in df.dtypes.items()}
            self.row_count = len(df)
            self.shape = df.shape
            self.available_columns = set(df.columns)
    
    def __repr__(self):
        return f"TableState(shape={self.shape}, cols={len(self.columns)})"


# ==================== MCTS Node ====================

@dataclass
class MCTSNode:
    """
    Node in MCTS planning tree.
    Tracks visit counts and Q-values for UCB calculation.
    Does NOT contain code or execution results.
    """
    operator_name: str
    parent: Optional['MCTSNode'] = None
    children: List['MCTSNode'] = field(default_factory=list)
    
    # MCTS statistics
    visit_count: int = 0
    total_reward: float = 0.0
    q_value: float = 0.0
    prior_prob: float = 0.1  # Initial prior
    
    # State tracking (for dependency checking)
    available_state: Set[str] = field(default_factory=set)
    completed_ops: Set[str] = field(default_factory=set)
    
    def update(self, reward: float):
        """Update statistics after path evaluation"""
        self.visit_count += 1
        self.total_reward += reward
        self.q_value = self.total_reward / self.visit_count if self.visit_count > 0 else 0.0
    
    def ucb_score(self, parent_visits: int, exploration_weight: float = 1.0) -> float:
        """
        Calculate P-UCB score for selection.
        
        Formula:
            PUCB(s,a) = Q(s,a)/N(s,a) + c * P(a|s) * sqrt(N(s)) / (1 + N(s,a))
        
        Where:
            - Q(s,a) = total_reward (cumulative)
            - N(s,a) = visit_count
            - P(a|s) = prior_prob (from ADO or learned from SMG)
            - c = exploration_weight
            - N(s) = parent_visits
        """
        import math
        
        if self.visit_count == 0:
            return float('inf')  # Always explore unvisited nodes first
        
        # Exploitation term: Q/N (average reward)
        exploitation = self.q_value  # Already computed as total_reward / visit_count
        
        # Exploration term: c * P(a|s) * sqrt(N(s)) / (1 + N(s,a))
        exploration = (
            exploration_weight * 
            self.prior_prob * 
            math.sqrt(parent_visits) / 
            (1 + self.visit_count)
        )
        
        return exploitation + exploration
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "operator": self.operator_name,
            "visits": self.visit_count,
            "q_value": self.q_value,
            "total_reward": self.total_reward,
            "prior_prob": self.prior_prob,
            "available_state": list(self.available_state),
            "completed_ops": list(self.completed_ops)
        }
    
    def __repr__(self):
        return f"MCTSNode({self.operator_name}, V={self.visit_count}, Q={self.q_value:.3f})"


# ==================== SMG Node ====================

@dataclass
class SMGNode:
    """
    Node in Structured Memory Graph.
    Stores execution history, code, rewards, and state changes.
    """
    operator_name: str
    code: str = ""
    
    # Execution results
    success: bool = False
    error_message: str = ""
    
    # State tracking
    state_before: Optional[TableState] = None
    state_after: Optional[TableState] = None
    
    # Reward information
    reward_vector: Optional['RewardVector'] = None
    
    # Metadata
    execution_time: float = 0.0
    timestamp: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "operator": self.operator_name,
            "code": self.code[:200] + "..." if len(self.code) > 200 else self.code,
            "success": self.success,
            "error": self.error_message,
            "state_before": self.state_before.to_dict() if self.state_before else None,
            "state_after": self.state_after.to_dict() if self.state_after else None,
            "reward": self.reward_vector.to_dict() if self.reward_vector else None,
            "execution_time": self.execution_time,
            "timestamp": self.timestamp
        }
    
    def __repr__(self):
        status = "✓" if self.success else "✗"
        return f"SMGNode({self.operator_name} {status}, reward={self.reward_vector.total_score if self.reward_vector else 0:.2f})"


# ==================== Reward Vector ====================

@dataclass
class RewardVector:
    """
    Structured reward from LLM evaluation.
    Multiple dimensions for fine-grained feedback.
    """
    execution_success: float = 0.0  # 0-1: Did code execute without errors?
    query_satisfaction: float = 0.0  # 0-1: Does output help answer the query?
    code_reasonableness: float = 0.0  # 0-1: Is the code appropriate for the operator?
    efficiency: float = 0.0  # 0-1: Is the operation efficient?
    error_severity: float = 0.0  # 0-1: How severe is the error (if any)?
    
    # Combined score
    total_score: float = 0.0
    
    # LLM reasoning
    explanation: str = ""
    
    def compute_total(self, weights: Optional[Dict[str, float]] = None):
        """
        Compute weighted total score.
        
        CRITICAL: LLM does NOT participate in weighting.
        LLM only provides raw sub-reward scores.
        System-level aggregation is done here.
        """
        if weights is None:
            # Default weights (you can adjust these)
            weights = {
                "execution_success": 2.0,
                "query_satisfaction": 3.0,
                "code_reasonableness": 1.5,
                "efficiency": 1.0,
                "error_severity": -3.0  # Negative weight = penalty
            }
        
        self.total_score = (
            weights["execution_success"] * self.execution_success +
            weights["query_satisfaction"] * self.query_satisfaction +
            weights["code_reasonableness"] * self.code_reasonableness +
            weights["efficiency"] * self.efficiency +
            weights["error_severity"] * self.error_severity  # Already negative weight
        )
        return self.total_score
    
    @staticmethod
    def aggregate_reward(r: Dict[str, float], weights: Optional[Dict[str, float]] = None) -> float:
        """
        Aggregate sub-rewards into a single scalar reward.
        
        This is the system-level aggregation function.
        LLM does NOT participate in this weighting.
        
        Args:
            r: Dict with keys: execution_success, query_satisfaction, 
               code_reasonableness, efficiency, error_severity
            weights: Optional custom weights
        
        Returns:
            Aggregated reward score
        
        Example:
            r = {
                "execution_success": 1,
                "query_satisfaction": 0.8,
                "code_reasonableness": 0.9,
                "efficiency": 0.7,
                "error_severity": 0.0
            }
            reward = RewardVector.aggregate_reward(r)
            # => 2.0*1 + 3.0*0.8 + 1.5*0.9 + 1.0*0.7 - 3.0*0.0 = 6.45
        """
        if weights is None:
            weights = {
                "execution_success": 2.0,
                "query_satisfaction": 3.0,
                "code_reasonableness": 1.5,
                "efficiency": 1.0,
                "error_severity": -3.0  # Penalty
            }
        
        return (
            weights["execution_success"] * r.get("execution_success", 0.0) +
            weights["query_satisfaction"] * r.get("query_satisfaction", 0.0) +
            weights["code_reasonableness"] * r.get("code_reasonableness", 0.0) +
            weights["efficiency"] * r.get("efficiency", 0.0) +
            weights["error_severity"] * r.get("error_severity", 0.0)
        )
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "execution_success": self.execution_success,
            "query_satisfaction": self.query_satisfaction,
            "code_reasonableness": self.code_reasonableness,
            "efficiency": self.efficiency,
            "error_severity": self.error_severity,
            "total_score": self.total_score,
            "explanation": self.explanation
        }
    
    @staticmethod
    def from_dict(data: Dict[str, Any]) -> 'RewardVector':
        """Create RewardVector from dict"""
        rv = RewardVector()
        rv.execution_success = data.get("execution_success", 0.0)
        rv.query_satisfaction = data.get("query_satisfaction", 0.0)
        rv.code_reasonableness = data.get("code_reasonableness", 0.0)
        rv.efficiency = data.get("efficiency", 0.0)
        rv.error_severity = data.get("error_severity", 0.0)
        rv.total_score = data.get("total_score", 0.0)
        rv.explanation = data.get("explanation", "")
        return rv
    
    def __repr__(self):
        return f"RewardVector(total={self.total_score:.2f}, exec={self.execution_success:.1f}, query={self.query_satisfaction:.1f})"


# ==================== ADO Result ====================

@dataclass
class ADOResult:
    """
    Output from ADO module.
    Contains a SET of operators (unordered).
    """
    operators: List[Operator] = field(default_factory=list)
    user_query: str = ""
    table_metadata: Dict[str, Any] = field(default_factory=dict)
    raw_llm_response: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "user_query": self.user_query,
            "operator_count": len(self.operators),
            "operators": [op.to_dict() for op in self.operators],
            "table_columns": self.table_metadata.get("column_names", [])
        }
    
    def __repr__(self):
        return f"ADOResult({len(self.operators)} operators)"


# ==================== Helper Functions ====================

def serialize_to_json(obj: Any, indent: int = 2) -> str:
    """Serialize any DTR object to JSON string"""
    if hasattr(obj, 'to_dict'):
        return json.dumps(obj.to_dict(), indent=indent, ensure_ascii=False)
    return json.dumps(obj, indent=indent, ensure_ascii=False, default=str)


def log_structure(obj: Any, logger, prefix: str = ""):
    """Log any DTR structure for debugging"""
    if hasattr(obj, 'to_dict'):
        data = obj.to_dict()
        logger.info(f"{prefix}{type(obj).__name__}:")
        for key, value in data.items():
            if isinstance(value, (str, int, float, bool)):
                logger.info(f"  {key}: {value}")
            elif isinstance(value, list) and len(value) <= 5:
                logger.info(f"  {key}: {value}")
            else:
                logger.info(f"  {key}: <{type(value).__name__} with {len(value) if hasattr(value, '__len__') else '?'} items>")

