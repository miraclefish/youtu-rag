"""
ADG Benchmark Runner V4 - åˆ†æ­¥æ‰§è¡Œ + ä¸°å¯Œä¸Šä¸‹æ–‡
"""

import os
import sys
import json
import datetime
import argparse
import yaml
import asyncio
from tqdm import tqdm
from pathlib import Path
import concurrent.futures
from threading import Lock
from dataclasses import dataclass, field
from typing import Any, AsyncIterator, Literal

project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

# æ·»åŠ å½“å‰æ¨¡å—çš„çˆ¶ç›®å½•åˆ° sys.path
integration_root = Path(__file__).parent
sys.path.insert(0, str(integration_root))

from utils.llm_client import LLMClient
from src.modules.smg_autonomous import SMGAutonomousModule
from src.modules.multi_sheet_loader import MultiSheetLoader, MultiSheetContext
from src.modules.sheet_state_manager import SheetStateManager
from utils.logger import logger
from utils.smart_table_processor import SmartTableProcessor
from utils.meta_extractor import MetaExtractor

from utu.agents.common import TaskRecorder, QueueCompleteSentinel
from utu.tools.memory_toolkit import VectorMemoryToolkit



@dataclass
class ExcelAgentStreamEvent:
    """ExcelAgent æµå¼äº‹ä»¶"""
    name: Literal[
        "excel_agent.plan.start",
        "excel_agent.plan.delta",
        "excel_agent.plan.done",
        "excel_agent.task.start",
        "excel_agent.task.delta",
        "excel_agent.task.done",
        "excel_agent.answer.start",
        "excel_agent.answer.delta",
        "excel_agent.answer.done",
    ]
    item: dict | None = None
    type: Literal["excel_agent_stream_event"] = "excel_agent_stream_event"


@dataclass
class ExcelAgentRecorder(TaskRecorder):
    """ç”¨äºè®°å½•å’Œæµå¼ä¼ è¾“ ExcelAgent çš„æ‰§è¡Œç»“æœ
    
    ç»§æ‰¿è‡ª TaskRecorderï¼Œä¿æŒä¸å…¶ä»– Agent æ¥å£ä¸€è‡´
    """
    # ExcelAgent ç‰¹æœ‰å­—æ®µ
    question_type: str = ""
    execution_trace: list = field(default_factory=list)


class ExcelAgent:

    def __init__(self, config):
        self.config = self._load_config(config)
        # workflow å’Œ instance å°†åœ¨è¿è¡Œæ—¶æ ¹æ® event_callback åˆ›å»º
        self.workflow = None
        self.instance = None
        self._memory_toolkit = None

    def set_memory_toolkit(self, memory_toolkit: "VectorMemoryToolkit") -> None:
        """Set the memory toolkit for this agent.

        Args:
            memory_toolkit: VectorMemoryToolkit instance for memory operations.
        """
        self._memory_toolkit = memory_toolkit

    @property
    def memory_toolkit(self) -> "VectorMemoryToolkit | None":
        """Get the memory toolkit if set."""
        return self._memory_toolkit

    def _load_config(self, config):
        with open(config, 'r') as f:
            return yaml.safe_load(f)
    
    def _build_workflow(self, event_callback=None):
        """æ„å»º workflowï¼Œæ”¯æŒä¼ å…¥äº‹ä»¶å›è°ƒ"""
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        llm_client = LLMClient(
            model=os.getenv("UTU_LLM_MODEL", "deepseek-v3"),
            base_url=os.getenv("UTU_LLM_BASE_URL", "https://api.lkeap.cloud.tencent.com/v1"),
            api_key=os.getenv("UTU_LLM_API_KEY", ""),
            temperature=0.0,
            max_tokens=4096
        )

        smg = SMGAutonomousModule(llm_client, event_callback=event_callback, reward_evaluator=None)
        # é‡ç½®tokenè®¡æ•°
        llm_client.reset_call_count()
        return smg



    async def run(self, input, question_type=None):
        """è¿è¡ŒæŸ¥è¯¢ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        recorder = self.run_streamed(input, question_type)
        async for _ in recorder.stream_events():
            pass
        return {
            'question': recorder.task,
            'FileName': self._get_file_name(),
            'model_answer': recorder.final_output,
        }
    
    def load_data(self, table_file):
        """åŠ è½½æ•°æ®ï¼ˆæ”¯æŒå¤šsheetï¼‰
        
        Returns:
            (multi_sheet_context, table_info_str)
        """
        import pandas as pd
        logger.info(f"ğŸ“‚ åŠ è½½è¡¨æ ¼: {table_file}")
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        if table_file.suffix not in ['.xlsx', '.xls', '.csv']:
            raise ValueError(f"Unsupported file type: {table_file.suffix}")
        
        # CSVæ–‡ä»¶ï¼šè½¬æ¢ä¸ºå•sheetçš„MultiSheetContext
        if table_file.suffix == '.csv':
            df = pd.read_csv(table_file)
            
            if df is None or df.empty:
                logger.error("âŒ è¡¨æ ¼åŠ è½½å¤±è´¥æˆ–ä¸ºç©º")
                return None, None
            
            # åˆ›å»ºå•sheetçš„context
            from src.modules.multi_sheet_loader import SheetState, MultiSheetContext
            
            state = SheetState(
                name="Sheet1",
                original_df=df.copy(),
                current_df=df.copy(),
                metadata={
                    "sheet_name": "Sheet1",
                    "shape": df.shape,
                    "columns": list(df.columns),
                    "dtypes": {col: str(dtype) for col, dtype in df.dtypes.items()}
                }
            )
            
            context = MultiSheetContext(
                file_path=str(table_file),
                sheet_states={"Sheet1": state},
                default_sheet="Sheet1",
                total_sheets=1
            )
            
            # ç”Ÿæˆè¡¨æ ¼ä¿¡æ¯
            table_info = self._generate_table_info(context)
            return context, table_info
        
        # Excelæ–‡ä»¶ï¼šä½¿ç”¨MultiSheetLoaderåŠ è½½
        try:
            # åˆå§‹åŒ–å¤„ç†å™¨
            processor = SmartTableProcessor()
            meta_extractor = MetaExtractor()
            
            # åŠ è½½æ‰€æœ‰sheet
            loader = MultiSheetLoader(max_preview_rows=6)
            context = loader.load_excel_file(
                str(table_file),
                processor=processor,
                meta_extractor=meta_extractor
            )
            
            # ç”Ÿæˆè¡¨æ ¼ä¿¡æ¯
            table_info = self._generate_table_info(context)
            
            return context, table_info
            
        except Exception as e:
            logger.error(f"âŒ å¤šsheetåŠ è½½å¤±è´¥: {e}")
            # Fallback: å°è¯•ç®€å•åŠ è½½
            try:
                df = pd.read_excel(table_file)
                
                from src.modules.multi_sheet_loader import SheetState, MultiSheetContext
                
                state = SheetState(
                    name="Sheet1",
                    original_df=df.copy(),
                    current_df=df.copy(),
                    metadata={
                        "sheet_name": "Sheet1",
                        "shape": df.shape,
                        "columns": list(df.columns),
                        "dtypes": {col: str(dtype) for col, dtype in df.dtypes.items()}
                    }
                )
                
                context = MultiSheetContext(
                    file_path=str(table_file),
                    sheet_states={"Sheet1": state},
                    default_sheet="Sheet1",
                    total_sheets=1
                )
                
                table_info = self._generate_table_info(context)
                return context, table_info
                
            except Exception as e2:
                logger.error(f"âŒ FallbackåŠ è½½ä¹Ÿå¤±è´¥: {e2}")
                return None, None
    
    def _generate_table_info(self, context: MultiSheetContext) -> str:
        """ç”Ÿæˆè¡¨æ ¼ä¿¡æ¯å­—ç¬¦ä¸²ï¼ˆç”¨äºæ—¥å¿—ï¼‰"""
        lines = []
        lines.append(f"ğŸ“ æ–‡ä»¶: {Path(context.file_path).name}")
        lines.append(f"ğŸ“Š Sheets: {context.total_sheets}")
        
        for sheet_name, state in context.sheet_states.items():
            df = state.current_df
            prefix = "â†’" if sheet_name == context.default_sheet else " "
            lines.append(f"\n{prefix} Sheet '{sheet_name}':")
            lines.append(f"  ç»´åº¦: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
            lines.append(f"  åˆ—å: {', '.join(df.columns.tolist()[:10])}")
            if len(df.columns) > 10:
                lines.append(f"       ... ({len(df.columns) - 10} more columns)")
            
            lines.append(f"\n  æ•°æ®ç±»å‹:")
            for col, dtype in list(df.dtypes.items())[:5]:
                lines.append(f"    â€¢ {col}: {dtype}")
            if len(df.dtypes) > 5:
                lines.append(f"    ... ({len(df.dtypes) - 5} more columns)")
            
            lines.append(f"\n  æ•°æ®é¢„è§ˆ (å‰5è¡Œ):")
            lines.append(df.head().to_string())
        
        return "\n".join(lines)

    def run_streamed(self, input, question_type=None, use_memory: bool = True) -> ExcelAgentRecorder:
        """æµå¼è¿è¡ŒæŸ¥è¯¢"""
        recorder = ExcelAgentRecorder(task=input, question_type=question_type, trace_id="")
        recorder._run_impl_task = asyncio.create_task(self._start_streaming(recorder, use_memory=use_memory))
        return recorder
    
    def _get_file_name(self):
        """è·å–æ–‡ä»¶å"""
        file_path = os.environ.get("FILE_PATH", None)
        if file_path:
            return Path(file_path.split(",")[0]).stem
        return "unknown"
    
    async def _start_streaming(self, recorder: ExcelAgentRecorder, use_memory: bool = True):
        """å¼‚æ­¥æ‰§è¡Œæµç¨‹"""
        try:
            # ä»ç¯å¢ƒå˜é‡è¯»å–å…¨å±€é…ç½®ï¼Œè¦†ç›–ä¼ å…¥å‚æ•°
            env_memory_setting = os.environ.get("memoryEnabled", "false").lower() == "true"
            use_memory = env_memory_setting
            logger.info(f"[ExcelAgent] use_memory from env: {use_memory}")
            logger.info(f"[ExcelAgent] self._memory_toolkit: {self._memory_toolkit}")
            question = recorder.task
            question_type = recorder.question_type
            original_question = question  # ä¿å­˜åŸå§‹é—®é¢˜ç”¨äº episodic memory

            if use_memory:
                logger.info(f"[ExcelAgent] use_memory: {use_memory}")

            if use_memory and self._memory_toolkit:
                await self._memory_toolkit.store_working_memory(question, role="user")
                logger.debug("Stored user question to working memory")

                # ä½¿ç”¨ç»Ÿä¸€çš„ memory æ£€ç´¢æ–¹æ³•
                memory_contexts = await self._memory_toolkit.retrieve_all_context(
                    query=question,
                    include_skills=False,
                )
                memory_context = memory_contexts["memory_context"]

                if memory_context:
                    logger.info(f"Retrieved memory context: {len(memory_context)} chars")

                enhanced_input = f"# ç›¸å…³å†å²ä¸Šä¸‹æ–‡\n{memory_context}\n\n---\n# å½“å‰é—®é¢˜\n{question}"
                recorder.task = enhanced_input
                question = enhanced_input

            file_path = os.environ.get("FILE_PATH", None)
            file_path = Path(file_path.split(",")[0])
            file_name = file_path.stem
            
            # è·å–å½“å‰äº‹ä»¶å¾ªç¯ï¼Œä¾›å›è°ƒå‡½æ•°ä½¿ç”¨
            current_loop = asyncio.get_running_loop()
            
            # å®šä¹‰çº¿ç¨‹å®‰å…¨çš„äº‹ä»¶å›è°ƒå‡½æ•°
            def event_callback(name: str, event_data: dict):
                """æ¥æ”¶æ¥è‡ª workflow çš„äº‹ä»¶å¹¶è½¬å‘ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
                try:
                    event = ExcelAgentStreamEvent(
                        name=name,
                        item=event_data
                    )
                    # ä½¿ç”¨ call_soon_threadsafe ç¡®ä¿è·¨çº¿ç¨‹å®‰å…¨
                    current_loop.call_soon_threadsafe(
                        recorder._event_queue.put_nowait,
                        event
                    )
                    logger.debug(f"Event sent from thread: {event_data.get('step', 'unknown')}")
                except Exception as e:
                    logger.warning(f"Failed to send event from callback: {e}")
            

            # å‘é€å¼€å§‹äº‹ä»¶
            recorder._event_queue.put_nowait(
                ExcelAgentStreamEvent(
                    name="excel_agent.plan.start",
                    item={
                        "question": question,
                        "file_path": str(file_path)
                    }
                )
            )

            recorder._event_queue.put_nowait(
                ExcelAgentStreamEvent(
                    name="excel_agent.plan.delta",
                    item={
                        "content": "Loading data..."
                    }
                )
            )

            # åŠ è½½æ•°æ®ï¼ˆæ”¯æŒå¤šsheetï¼‰
            context, table_info = await asyncio.to_thread(self.load_data, file_path)
            
            if context is None:
                raise ValueError("Failed to load data")
            
            recorder._event_queue.put_nowait(
                ExcelAgentStreamEvent(
                    name="excel_agent.plan.delta",
                    item={
                        "content": table_info,
                        "clean": True
                    }
                )
            )

            # æ„å»ºmetadata
            sub_q_type = question_type or ""  # å¦‚æœæ²¡æœ‰æä¾›ï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
            max_iterations = self.config.get('config', {}).get('max_iterations', 10)  # ä»é…ç½®è¯»å–ï¼Œé»˜è®¤10
            
            # ä»contextæå–metadataï¼ˆä½¿ç”¨é»˜è®¤sheetçš„ä¿¡æ¯ï¼‰
            default_state = context.get_state(context.default_sheet)
            df = default_state.current_df
            
            metadata = {
                "column_names": list(df.columns),
                "column_types": {col: str(dtype) for col, dtype in df.dtypes.items()},
                "row_count": len(df),
                "shape": df.shape,
                "question_type": question_type,
                "sub_q_type": sub_q_type,
                # æ·»åŠ å¤šsheetä¿¡æ¯
                "total_sheets": context.total_sheets,
                "sheet_names": context.get_sheet_names(),
                "default_sheet": context.default_sheet
            }

            smg = self._build_workflow(event_callback=event_callback)

            try:
                result = await current_loop.run_in_executor(
                    None,
                    lambda: smg.execute_with_autonomous_loop(
                        operator_sequence=[],
                        operator_pool=[],
                        sheet_context=context,  # ä¼ é€’MultiSheetContext
                        user_query=question,
                        table_metadata=metadata,
                        schema_result=None,
                        max_iterations=max_iterations
                    )
                )
                final_answer = result.get("final_answer", "")
                execution_trace = result.get("execution_trace", [])
            except Exception as e:
                if "Recursion limit" in str(e):
                    logger.warning(f"Query hit recursion limit.")
                    final_answer = "[Final Answer]: Processing timeout"
                    execution_trace = []
                else:
                    raise e
            
            # æ›´æ–° recorder
            recorder.final_output = final_answer
            recorder.execution_trace = execution_trace

            final_output = str(recorder.final_output or "")
            logger.debug(f"Final output: {final_output}")

            # if use_memory and self._memory_toolkit:
            #     # å­˜å‚¨ working memory
            #     await self._memory_toolkit.store_working_memory(final_output, role="assistant")
            #     logger.debug("Saved model output to memory")

            # å­˜å‚¨åˆ° Memoryï¼ˆåŒ…æ‹¬ episodic memoryï¼‰
            if use_memory and self._memory_toolkit:
                try:
                    # å­˜å‚¨ working memory
                    await self._memory_toolkit.store_working_memory(final_output, role="assistant")
                    
                    # å­˜å‚¨åˆ° episodic memoryï¼ˆæŒä¹…åŒ–ï¼‰
                    # æ¢å¤åŸå§‹é—®é¢˜ï¼ˆå»é™¤ä¸Šä¸‹æ–‡æ³¨å…¥éƒ¨åˆ†ï¼‰
                    clean_question = original_question
                    if "\n# å½“å‰é—®é¢˜\n" in str(recorder.task):
                        clean_question = str(recorder.task).split("\n# å½“å‰é—®é¢˜\n")[-1]
                    
                    await self._memory_toolkit.save_conversation_to_episodic(
                        question=clean_question,
                        answer=final_output,
                        importance_score=0.6,  # Excel åˆ†æé€šå¸¸æ¯”è¾ƒé‡è¦
                    )
                    logger.debug("Saved conversation to episodic memory")
                except Exception as e:
                    logger.warning(f"Memory storage error: {e}")
            
            # å‘é€å®Œæˆäº‹ä»¶
            recorder._event_queue.put_nowait(
                ExcelAgentStreamEvent(
                    name="excel_agent.answer.delta",
                    item={
                        "type": "answer_generation",
                        "content": recorder.final_output
                    }
                )
            )
            
        except Exception as e:
            logger.error(f"Error processing task: {str(e)}")
            recorder._event_queue.put_nowait(QueueCompleteSentinel())
            recorder._is_complete = True
            raise e
        finally:
            recorder._event_queue.put_nowait(QueueCompleteSentinel())
            recorder._is_complete = True

if __name__ == "__main__":

    async def main():
        query = "/Users/felix/Documents/GitProjects/YoutuRAG_Benchmark/data/data_0109/å¤šè¡¨mini/excels/å¥¥è¿ä¼šå‚èµ›é˜Ÿä¼.xlsx"
        agent = ExcelAgent(config="configs/agents/ragref/excel/excel.yaml")
        
        # ä½¿ç”¨å¼‚æ­¥æµå¼è°ƒç”¨
        rec = agent.run_streamed(query)
        async for event in rec.stream_events():
            print(f"Event: {event}")
        
        print(f"\nFinal Answer:\n{rec.final_output}")
    
    asyncio.run(main())

